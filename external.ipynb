{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Body ID                                        articleBody\n",
      "0           0  A small meteorite crashed into a wooded area i...\n",
      "1           4  Last week we hinted at what was to come as Ebo...\n",
      "2           5  (NEWSER) – Wonder how long a Quarter Pounder w...\n",
      "3           6  Posting photos of a gun-toting child online, I...\n",
      "4           7  At least 25 suspected Boko Haram insurgents we...\n",
      "...       ...                                                ...\n",
      "1678     2528  Intelligence agencies hunting for identity of ...\n",
      "1679     2529  While Daleks \"know no fear\" and \"must not fear...\n",
      "1680     2530  More than 200 schoolgirls were kidnapped in Ap...\n",
      "1681     2531  A Guantanamo Bay prisoner released last year a...\n",
      "1682     2532  ANN ARBOR, Mich. – A pizza delivery man in Mic...\n",
      "\n",
      "[1683 rows x 2 columns]\n",
      "                                                Headline  Body ID     Stance\n",
      "0      Police find mass graves with at least '15 bodi...      712  unrelated\n",
      "1      Hundreds of Palestinians flee floods in Gaza a...      158      agree\n",
      "2      Christian Bale passes on role of Steve Jobs, a...      137  unrelated\n",
      "3      HBO and Apple in Talks for $15/Month Apple TV ...     1034  unrelated\n",
      "4      Spider burrowed through tourist's stomach and ...     1923   disagree\n",
      "...                                                  ...      ...        ...\n",
      "49967  Urgent: The Leader of ISIL 'Abu Bakr al-Baghda...     1681  unrelated\n",
      "49968  Brian Williams slams social media for speculat...     2419  unrelated\n",
      "49969  Mexico Says Missing Students Not Found In Firs...     1156      agree\n",
      "49970  US Lawmaker: Ten ISIS Fighters Have Been Appre...     1012    discuss\n",
      "49971  Shots Heard In Alleged Brown Shooting Recordin...     2044  unrelated\n",
      "\n",
      "[49972 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('train_bodies.csv')\n",
    "print(df1)\n",
    "df2= pd.read_csv('train_stances.csv')\n",
    "\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 Soldier shot, Parliament locked down after gunfire erupts at war memorial unrelated\n",
      "0 0 Tourist dubbed ‘Spider Man’ after spider burrows under skin for days unrelated\n",
      "0 0 Luke Somers 'killed in failed rescue attempt in Yemen' unrelated\n",
      "0 0 BREAKING: Soldier shot at War Memorial in Ottawa unrelated\n",
      "0 0 Giant 8ft 9in catfish weighing 19 stone caught in Italy is thought to be the biggest ever reeled in with a rod and line unrelated\n",
      "0 0 Enormous 20-stone catfish caught with fishing rod in Italy after 40-minute boat battle unrelated\n",
      "0 0 Italian catches huge wels catfish; is it a record? unrelated\n",
      "0 0 Not coming to a store near you: The pumpkin spice condom unrelated\n",
      "0 0 One gunman killed in shooting on Parliament Hill in Ottawa, hunt on for other shooters unrelated\n",
      "0 0 Soldier shot at war memorial in Canada unrelated\n",
      "0 0 Surreal Photos of Fisherman’s Jaw-Dropping Catch Will Likely Have People Wondering If It’s Real unrelated\n",
      "0 0 Fisherman lands 19 STONE catfish which could be biggest in world to be hooked unrelated\n",
      "0 0 Source: Tom Brokaw Wants Brian Williams Fired unrelated\n",
      "0 0 A soldier has been shot at Canada’s war memorial just steps away from the nation’s parliament unrelated\n",
      "0 0 280 Pound Catfish: Fisherman Makes Huge Catch In Italy, Catfish Could Set Record unrelated\n",
      "0 0 Rumor debunked: RoboCop-style robots are not patrolling Microsoft's campus unrelated\n",
      "0 0 Caught a catfish record in Po: 127 kg and 2.67 meters unrelated\n",
      "0 0 Monster catfish which looks big enough to swallow a man whole caught in Italy unrelated\n",
      "0 0 Luke Somers' sister says he was killed in failed Yemen rescue attempt unrelated\n",
      "0 0 Apple Watch to Be Shower-Proof, Have 100,000 Apps at Launch: Reports unrelated\n"
     ]
    }
   ],
   "source": [
    "train_stances_id=df2['Body ID']\n",
    "train_bodies_id=df1['Body ID']\n",
    "train_data=[]\n",
    "for each in train_bodies_id:\n",
    "    if each in train_stances_id:\n",
    "\n",
    "        target_rows1 = df1[df1['Body ID'] == each]\n",
    "        target_rows2 = df2[df2['Body ID'] == each]\n",
    "        for index1, row1 in target_rows1.iterrows():\n",
    "            for index2, row2 in target_rows2.iterrows():\n",
    "                train_data.append([row1[\"Body ID\"],row1['articleBody'],row2['Body ID'],row2['Headline'],row2['Stance']])\n",
    "                \n",
    "\n",
    "for i in range(20):\n",
    "    print(train_data[i][0],train_data[i][2],train_data[i][3],train_data[i][4])\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       bodies Body ID                                               body  \\\n",
      "0                   0  A small meteorite crashed into a wooded area i...   \n",
      "1                   0  A small meteorite crashed into a wooded area i...   \n",
      "2                   0  A small meteorite crashed into a wooded area i...   \n",
      "3                   0  A small meteorite crashed into a wooded area i...   \n",
      "4                   0  A small meteorite crashed into a wooded area i...   \n",
      "...               ...                                                ...   \n",
      "49967            2532  ANN ARBOR, Mich. – A pizza delivery man in Mic...   \n",
      "49968            2532  ANN ARBOR, Mich. – A pizza delivery man in Mic...   \n",
      "49969            2532  ANN ARBOR, Mich. – A pizza delivery man in Mic...   \n",
      "49970            2532  ANN ARBOR, Mich. – A pizza delivery man in Mic...   \n",
      "49971            2532  ANN ARBOR, Mich. – A pizza delivery man in Mic...   \n",
      "\n",
      "       Stances Body ID                                           headline  \\\n",
      "0                    0  Soldier shot, Parliament locked down after gun...   \n",
      "1                    0  Tourist dubbed ‘Spider Man’ after spider burro...   \n",
      "2                    0  Luke Somers 'killed in failed rescue attempt i...   \n",
      "3                    0   BREAKING: Soldier shot at War Memorial in Ottawa   \n",
      "4                    0  Giant 8ft 9in catfish weighing 19 stone caught...   \n",
      "...                ...                                                ...   \n",
      "49967             2532  Pizza delivery man gets tipped more than $2,00...   \n",
      "49968             2532                 Pizza delivery man gets $2,000 tip   \n",
      "49969             2532   Luckiest Pizza Delivery Guy Ever Gets $2,000 Tip   \n",
      "49970             2532  Ann Arbor pizza delivery driver surprised with...   \n",
      "49971             2532  Ann Arbor pizza delivery driver surprised with...   \n",
      "\n",
      "          Stance  \n",
      "0      unrelated  \n",
      "1      unrelated  \n",
      "2      unrelated  \n",
      "3      unrelated  \n",
      "4      unrelated  \n",
      "...          ...  \n",
      "49967      agree  \n",
      "49968      agree  \n",
      "49969      agree  \n",
      "49970      agree  \n",
      "49971      agree  \n",
      "\n",
      "[49972 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "df=pd.DataFrame(train_data,columns=['bodies Body ID','body','Stances Body ID','headline','Stance'])\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unrelated\n",
      "[52, 32, 10, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "15\n",
      "[0, 0, 0, 0, 0]\n",
      "5\n",
      "0.0\n",
      "0.20589999999999997\n",
      "[52, 32, 10, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.20589999999999997]\n"
     ]
    }
   ],
   "source": [
    "# Define the functions to compute external features\n",
    "s1=df.loc[0,'body']\n",
    "s2=df.loc[0,'headline']\n",
    "print(df.loc[0,'Stance'])\n",
    "\n",
    "def ngram_match(s1, s2, n):\n",
    "    \"\"\"Compute the number of n-grams matched between two strings\"\"\"\n",
    "    ngrams1 = set([s1[i:i+n] for i in range(len(s1)-n+1)])\n",
    "    ngrams2 = set([s2[i:i+n] for i in range(len(s2)-n+1)])\n",
    "    return len(ngrams1.intersection(ngrams2))\n",
    "\n",
    "def char_ngram_match(s1, s2):\n",
    "    \"\"\"Compute the number of character n-grams matched between two strings\"\"\"\n",
    "    match = []\n",
    "    for n in range(2, 17):\n",
    "        match .append(ngram_match(s1, s2, n))\n",
    "    return match\n",
    "print(char_ngram_match(s1,s2))\n",
    "print(len(char_ngram_match(s1,s2)))\n",
    "\n",
    "def word_ngram_match(s1, s2):\n",
    "    \"\"\"Compute the number of word n-grams matched between two strings\"\"\"\n",
    "    match = []\n",
    "    for n in range(2, 7):\n",
    "        s1_tokens = word_tokenize(s1)\n",
    "        s2_tokens = word_tokenize(s2)\n",
    "        s1_ngrams = set([' '.join(s1_tokens[i:i+n]) for i in range(len(s1_tokens)-n+1)])\n",
    "        s2_ngrams = set([' '.join(s2_tokens[i:i+n]) for i in range(len(s2_tokens)-n+1)])\n",
    "        match.append(len(s1_ngrams.intersection(s2_ngrams)))\n",
    "    return match\n",
    "\n",
    "print(word_ngram_match(s1,s2))\n",
    "print(len(word_ngram_match(s1,s2)))\n",
    "def tfidf_score(s1, s2):\n",
    "    \"\"\"Compute the weighted TF-IDF score between two strings\"\"\"\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf = vectorizer.fit_transform([s1, s2])\n",
    "    return (tfidf * tfidf.T).A[0,1]\n",
    "print(tfidf_score(s1,s2))\n",
    "\n",
    "def sentiment_difference(s1, s2):\n",
    "    \"\"\"Compute the sentiment difference between two strings\"\"\"\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    s1_sentiment = sid.polarity_scores(s1)['compound']\n",
    "    s2_sentiment = sid.polarity_scores(s2)['compound']\n",
    "    return abs(s1_sentiment - s2_sentiment)\n",
    "\n",
    "print(sentiment_difference(s1,s2))\n",
    "external_feature=char_ngram_match(s1,s2)+word_ngram_match(s1,s2)+[tfidf_score(s1,s2)]+[sentiment_difference(s1,s2)]\n",
    "print(external_feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        unrelated\n",
      "1        unrelated\n",
      "2        unrelated\n",
      "3        unrelated\n",
      "4        unrelated\n",
      "           ...    \n",
      "49967        agree\n",
      "49968        agree\n",
      "49969        agree\n",
      "49970        agree\n",
      "49971        agree\n",
      "Name: Stance, Length: 49972, dtype: object\n",
      "{'discuss', 'disagree', 'unrelated', 'agree'}\n",
      "0        [0, 0, 1, 0]\n",
      "1        [0, 0, 1, 0]\n",
      "2        [0, 0, 1, 0]\n",
      "3        [0, 0, 1, 0]\n",
      "4        [0, 0, 1, 0]\n",
      "             ...     \n",
      "49967    [0, 0, 0, 1]\n",
      "49968    [0, 0, 0, 1]\n",
      "49969    [0, 0, 0, 1]\n",
      "49970    [0, 0, 0, 1]\n",
      "49971    [0, 0, 0, 1]\n",
      "Name: Stance, Length: 49972, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "y = copy.deepcopy(df['Stance'])\n",
    "print(y)\n",
    "print(set(y))\n",
    "for index, value in y.iteritems():\n",
    "    if value==\"unrelated\":\n",
    "        y[index]=[0,0,1,0]\n",
    "    elif value==\"disagree\":\n",
    "        y[index]=[0,1,0,0]\n",
    "    elif value==\"discuss\":\n",
    "        y[index]=[1,0,0,0]\n",
    "    elif value==\"agree\":\n",
    "        y[index]=[0,0,0,1]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Define the functions to compute external features\n",
    "def ngram_match(s1, s2, n):\n",
    "    \"\"\"Compute the number of n-grams matched between two strings\"\"\"\n",
    "    ngrams1 = set([s1[i:i+n] for i in range(len(s1)-n+1)])\n",
    "    ngrams2 = set([s2[i:i+n] for i in range(len(s2)-n+1)])\n",
    "    return len(ngrams1.intersection(ngrams2))\n",
    "\n",
    "def char_ngram_match(s1, s2):\n",
    "    \"\"\"Compute the number of character n-grams matched between two strings\"\"\"\n",
    "    match = []\n",
    "    for n in range(2, 17):\n",
    "        match .append(ngram_match(s1, s2, n))\n",
    "    return match\n",
    "\n",
    "def word_ngram_match(s1, s2):\n",
    "    \"\"\"Compute the number of word n-grams matched between two strings\"\"\"\n",
    "    match = []\n",
    "    for n in range(2, 7):\n",
    "        s1_tokens = word_tokenize(s1)\n",
    "        s2_tokens = word_tokenize(s2)\n",
    "        s1_ngrams = set([' '.join(s1_tokens[i:i+n]) for i in range(len(s1_tokens)-n+1)])\n",
    "        s2_ngrams = set([' '.join(s2_tokens[i:i+n]) for i in range(len(s2_tokens)-n+1)])\n",
    "        match.append(len(s1_ngrams.intersection(s2_ngrams)))\n",
    "    return match\n",
    "\n",
    "def tfidf_score(s1, s2):\n",
    "    \"\"\"Compute the weighted TF-IDF score between two strings\"\"\"\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf = vectorizer.fit_transform([s1, s2])\n",
    "    return (tfidf * tfidf.T).A[0,1]\n",
    "\n",
    "def sentiment_difference(s1, s2):\n",
    "    \"\"\"Compute the sentiment difference between two strings\"\"\"\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    s1_sentiment = sid.polarity_scores(s1)['compound']\n",
    "    s2_sentiment = sid.polarity_scores(s2)['compound']\n",
    "    return abs(s1_sentiment - s2_sentiment)\n",
    "\n",
    "# Load the data as pandas dataframe\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Compute the external features and store in numpy array\n",
    "external_features =[]\n",
    "for i, row in df.iterrows():\n",
    "    s1 = row['headline']\n",
    "    s2 = row['body']\n",
    "    external_feature=char_ngram_match(s1,s2)+word_ngram_match(s1,s2)+[tfidf_score(s1,s2)]+[sentiment_difference(s1,s2)]\n",
    "    if i %1000==0:\n",
    "        print(\"num:{}, external_feature:{}\".format(i,external_feature))\n",
    "    external_features.append(external_feature)\n",
    "    # You can add more features here as needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[52, 32, 10, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.20589999999999997], [43, 28, 10, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.8053], [31, 16, 5, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.13480000000000003], [31, 18, 6, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.20589999999999997], [69, 64, 29, 14, 6, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0073264573132805115, 0.8053], [58, 39, 15, 6, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.42350000000000004], [33, 19, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 1.1235], [39, 29, 18, 6, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.022997222930638134, 0.8053], [56, 32, 14, 6, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.13480000000000003], [32, 21, 7, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.20589999999999997], [57, 25, 9, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.8053], [50, 32, 16, 11, 7, 5, 3, 2, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0.0, 0.8053], [27, 13, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.2479], [66, 50, 20, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.20589999999999997], [46, 21, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 1.1235], [51, 21, 6, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.8053], [35, 23, 11, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.2479], [59, 35, 19, 8, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.8053], [42, 28, 10, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.13480000000000003], [39, 17, 6, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.01850266147079386, 0.8053]]\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "\n",
    "print(external_features[:20])\n",
    "print(len(external_features[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=external_features)\n",
    "df.to_excel('external_features.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        [0, 0, 1, 0]\n",
      "1        [0, 0, 1, 0]\n",
      "2        [0, 0, 1, 0]\n",
      "3        [0, 0, 1, 0]\n",
      "4        [0, 0, 1, 0]\n",
      "             ...     \n",
      "49967    [0, 0, 0, 1]\n",
      "49968    [0, 0, 0, 1]\n",
      "49969    [0, 0, 0, 1]\n",
      "49970    [0, 0, 0, 1]\n",
      "49971    [0, 0, 0, 1]\n",
      "Name: Stance, Length: 49972, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the MLP model and pass the external features as input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=22))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categori_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model using the external features\n",
    "X = external_features\n",
    "#y = df['Stance']\n",
    "model.fit(X, y, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
