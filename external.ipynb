{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Body ID                                        articleBody\n",
      "0           0  A small meteorite crashed into a wooded area i...\n",
      "1           4  Last week we hinted at what was to come as Ebo...\n",
      "2           5  (NEWSER) – Wonder how long a Quarter Pounder w...\n",
      "3           6  Posting photos of a gun-toting child online, I...\n",
      "4           7  At least 25 suspected Boko Haram insurgents we...\n",
      "...       ...                                                ...\n",
      "1678     2528  Intelligence agencies hunting for identity of ...\n",
      "1679     2529  While Daleks \"know no fear\" and \"must not fear...\n",
      "1680     2530  More than 200 schoolgirls were kidnapped in Ap...\n",
      "1681     2531  A Guantanamo Bay prisoner released last year a...\n",
      "1682     2532  ANN ARBOR, Mich. – A pizza delivery man in Mic...\n",
      "\n",
      "[1683 rows x 2 columns]\n",
      "                                                Headline  Body ID     Stance\n",
      "0      Police find mass graves with at least '15 bodi...      712  unrelated\n",
      "1      Hundreds of Palestinians flee floods in Gaza a...      158      agree\n",
      "2      Christian Bale passes on role of Steve Jobs, a...      137  unrelated\n",
      "3      HBO and Apple in Talks for $15/Month Apple TV ...     1034  unrelated\n",
      "4      Spider burrowed through tourist's stomach and ...     1923   disagree\n",
      "...                                                  ...      ...        ...\n",
      "49967  Urgent: The Leader of ISIL 'Abu Bakr al-Baghda...     1681  unrelated\n",
      "49968  Brian Williams slams social media for speculat...     2419  unrelated\n",
      "49969  Mexico Says Missing Students Not Found In Firs...     1156      agree\n",
      "49970  US Lawmaker: Ten ISIS Fighters Have Been Appre...     1012    discuss\n",
      "49971  Shots Heard In Alleged Brown Shooting Recordin...     2044  unrelated\n",
      "\n",
      "[49972 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('train_bodies.csv')\n",
    "print(df1)\n",
    "df2= pd.read_csv('train_stances.csv')\n",
    "\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 Soldier shot, Parliament locked down after gunfire erupts at war memorial unrelated\n",
      "0 0 Tourist dubbed ‘Spider Man’ after spider burrows under skin for days unrelated\n",
      "0 0 Luke Somers 'killed in failed rescue attempt in Yemen' unrelated\n",
      "0 0 BREAKING: Soldier shot at War Memorial in Ottawa unrelated\n",
      "0 0 Giant 8ft 9in catfish weighing 19 stone caught in Italy is thought to be the biggest ever reeled in with a rod and line unrelated\n",
      "0 0 Enormous 20-stone catfish caught with fishing rod in Italy after 40-minute boat battle unrelated\n",
      "0 0 Italian catches huge wels catfish; is it a record? unrelated\n",
      "0 0 Not coming to a store near you: The pumpkin spice condom unrelated\n",
      "0 0 One gunman killed in shooting on Parliament Hill in Ottawa, hunt on for other shooters unrelated\n",
      "0 0 Soldier shot at war memorial in Canada unrelated\n",
      "0 0 Surreal Photos of Fisherman’s Jaw-Dropping Catch Will Likely Have People Wondering If It’s Real unrelated\n",
      "0 0 Fisherman lands 19 STONE catfish which could be biggest in world to be hooked unrelated\n",
      "0 0 Source: Tom Brokaw Wants Brian Williams Fired unrelated\n",
      "0 0 A soldier has been shot at Canada’s war memorial just steps away from the nation’s parliament unrelated\n",
      "0 0 280 Pound Catfish: Fisherman Makes Huge Catch In Italy, Catfish Could Set Record unrelated\n",
      "0 0 Rumor debunked: RoboCop-style robots are not patrolling Microsoft's campus unrelated\n",
      "0 0 Caught a catfish record in Po: 127 kg and 2.67 meters unrelated\n",
      "0 0 Monster catfish which looks big enough to swallow a man whole caught in Italy unrelated\n",
      "0 0 Luke Somers' sister says he was killed in failed Yemen rescue attempt unrelated\n",
      "0 0 Apple Watch to Be Shower-Proof, Have 100,000 Apps at Launch: Reports unrelated\n"
     ]
    }
   ],
   "source": [
    "train_stances_id=df2['Body ID']\n",
    "train_bodies_id=df1['Body ID']\n",
    "train_data=[]\n",
    "for each in train_bodies_id:\n",
    "    if each in train_stances_id:\n",
    "\n",
    "        target_rows1 = df1[df1['Body ID'] == each]\n",
    "        target_rows2 = df2[df2['Body ID'] == each]\n",
    "        for index1, row1 in target_rows1.iterrows():\n",
    "            for index2, row2 in target_rows2.iterrows():\n",
    "                train_data.append([row1[\"Body ID\"],row1['articleBody'],row2['Body ID'],row2['Headline'],row2['Stance']])\n",
    "                \n",
    "\n",
    "for i in range(20):\n",
    "    print(train_data[i][0],train_data[i][2],train_data[i][3],train_data[i][4])\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       bodies Body ID                                               body  \\\n",
      "0                   0  A small meteorite crashed into a wooded area i...   \n",
      "1                   0  A small meteorite crashed into a wooded area i...   \n",
      "2                   0  A small meteorite crashed into a wooded area i...   \n",
      "3                   0  A small meteorite crashed into a wooded area i...   \n",
      "4                   0  A small meteorite crashed into a wooded area i...   \n",
      "...               ...                                                ...   \n",
      "49967            2532  ANN ARBOR, Mich. – A pizza delivery man in Mic...   \n",
      "49968            2532  ANN ARBOR, Mich. – A pizza delivery man in Mic...   \n",
      "49969            2532  ANN ARBOR, Mich. – A pizza delivery man in Mic...   \n",
      "49970            2532  ANN ARBOR, Mich. – A pizza delivery man in Mic...   \n",
      "49971            2532  ANN ARBOR, Mich. – A pizza delivery man in Mic...   \n",
      "\n",
      "       Stances Body ID                                           headline  \\\n",
      "0                    0  Soldier shot, Parliament locked down after gun...   \n",
      "1                    0  Tourist dubbed ‘Spider Man’ after spider burro...   \n",
      "2                    0  Luke Somers 'killed in failed rescue attempt i...   \n",
      "3                    0   BREAKING: Soldier shot at War Memorial in Ottawa   \n",
      "4                    0  Giant 8ft 9in catfish weighing 19 stone caught...   \n",
      "...                ...                                                ...   \n",
      "49967             2532  Pizza delivery man gets tipped more than $2,00...   \n",
      "49968             2532                 Pizza delivery man gets $2,000 tip   \n",
      "49969             2532   Luckiest Pizza Delivery Guy Ever Gets $2,000 Tip   \n",
      "49970             2532  Ann Arbor pizza delivery driver surprised with...   \n",
      "49971             2532  Ann Arbor pizza delivery driver surprised with...   \n",
      "\n",
      "          Stance  \n",
      "0      unrelated  \n",
      "1      unrelated  \n",
      "2      unrelated  \n",
      "3      unrelated  \n",
      "4      unrelated  \n",
      "...          ...  \n",
      "49967      agree  \n",
      "49968      agree  \n",
      "49969      agree  \n",
      "49970      agree  \n",
      "49971      agree  \n",
      "\n",
      "[49972 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "df=pd.DataFrame(train_data,columns=['bodies Body ID','body','Stances Body ID','headline','Stance'])\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unrelated\n",
      "[52, 32, 10, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "15\n",
      "[0, 0, 0, 0, 0]\n",
      "5\n",
      "0.0\n",
      "0.20589999999999997\n",
      "[52, 32, 10, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.20589999999999997]\n"
     ]
    }
   ],
   "source": [
    "# Define the functions to compute external features\n",
    "s1=df.loc[0,'body']\n",
    "s2=df.loc[0,'headline']\n",
    "print(df.loc[0,'Stance'])\n",
    "\n",
    "def ngram_match(s1, s2, n):\n",
    "    \"\"\"Compute the number of n-grams matched between two strings\"\"\"\n",
    "    ngrams1 = set([s1[i:i+n] for i in range(len(s1)-n+1)])\n",
    "    ngrams2 = set([s2[i:i+n] for i in range(len(s2)-n+1)])\n",
    "    return len(ngrams1.intersection(ngrams2))\n",
    "\n",
    "def char_ngram_match(s1, s2):\n",
    "    \"\"\"Compute the number of character n-grams matched between two strings\"\"\"\n",
    "    match = []\n",
    "    for n in range(2, 17):\n",
    "        match .append(ngram_match(s1, s2, n))\n",
    "    return match\n",
    "print(char_ngram_match(s1,s2))\n",
    "print(len(char_ngram_match(s1,s2)))\n",
    "\n",
    "def word_ngram_match(s1, s2):\n",
    "    \"\"\"Compute the number of word n-grams matched between two strings\"\"\"\n",
    "    match = []\n",
    "    for n in range(2, 7):\n",
    "        s1_tokens = word_tokenize(s1)\n",
    "        s2_tokens = word_tokenize(s2)\n",
    "        s1_ngrams = set([' '.join(s1_tokens[i:i+n]) for i in range(len(s1_tokens)-n+1)])\n",
    "        s2_ngrams = set([' '.join(s2_tokens[i:i+n]) for i in range(len(s2_tokens)-n+1)])\n",
    "        match.append(len(s1_ngrams.intersection(s2_ngrams)))\n",
    "    return match\n",
    "\n",
    "print(word_ngram_match(s1,s2))\n",
    "print(len(word_ngram_match(s1,s2)))\n",
    "def tfidf_score(s1, s2):\n",
    "    \"\"\"Compute the weighted TF-IDF score between two strings\"\"\"\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf = vectorizer.fit_transform([s1, s2])\n",
    "    return (tfidf * tfidf.T).A[0,1]\n",
    "print(tfidf_score(s1,s2))\n",
    "\n",
    "def sentiment_difference(s1, s2):\n",
    "    \"\"\"Compute the sentiment difference between two strings\"\"\"\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    s1_sentiment = sid.polarity_scores(s1)['compound']\n",
    "    s2_sentiment = sid.polarity_scores(s2)['compound']\n",
    "    return abs(s1_sentiment - s2_sentiment)\n",
    "\n",
    "print(sentiment_difference(s1,s2))\n",
    "external_feature=char_ngram_match(s1,s2)+word_ngram_match(s1,s2)+[tfidf_score(s1,s2)]+[sentiment_difference(s1,s2)]\n",
    "print(external_feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        unrelated\n",
      "1        unrelated\n",
      "2        unrelated\n",
      "3        unrelated\n",
      "4        unrelated\n",
      "           ...    \n",
      "49967        agree\n",
      "49968        agree\n",
      "49969        agree\n",
      "49970        agree\n",
      "49971        agree\n",
      "Name: Stance, Length: 49972, dtype: object\n",
      "{'agree', 'discuss', 'unrelated', 'disagree'}\n",
      "0        [0, 0, 1, 0]\n",
      "1        [0, 0, 1, 0]\n",
      "2        [0, 0, 1, 0]\n",
      "3        [0, 0, 1, 0]\n",
      "4        [0, 0, 1, 0]\n",
      "             ...     \n",
      "49967    [0, 0, 0, 1]\n",
      "49968    [0, 0, 0, 1]\n",
      "49969    [0, 0, 0, 1]\n",
      "49970    [0, 0, 0, 1]\n",
      "49971    [0, 0, 0, 1]\n",
      "Name: Stance, Length: 49972, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "y = copy.deepcopy(df['Stance'])\n",
    "print(y)\n",
    "print(set(y))\n",
    "for index, value in y.iteritems():\n",
    "    if value==\"unrelated\":\n",
    "        y[index]=[0,0,1,0]\n",
    "    elif value==\"disagree\":\n",
    "        y[index]=[0,1,0,0]\n",
    "    elif value==\"discuss\":\n",
    "        y[index]=[1,0,0,0]\n",
    "    elif value==\"agree\":\n",
    "        y[index]=[0,0,0,1]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Define the functions to compute external features\n",
    "def ngram_match(s1, s2, n):\n",
    "    \"\"\"Compute the number of n-grams matched between two strings\"\"\"\n",
    "    ngrams1 = set([s1[i:i+n] for i in range(len(s1)-n+1)])\n",
    "    ngrams2 = set([s2[i:i+n] for i in range(len(s2)-n+1)])\n",
    "    return len(ngrams1.intersection(ngrams2))\n",
    "\n",
    "def char_ngram_match(s1, s2):\n",
    "    \"\"\"Compute the number of character n-grams matched between two strings\"\"\"\n",
    "    match = []\n",
    "    for n in range(2, 17):\n",
    "        match .append(ngram_match(s1, s2, n))\n",
    "    return match\n",
    "\n",
    "def word_ngram_match(s1, s2):\n",
    "    \"\"\"Compute the number of word n-grams matched between two strings\"\"\"\n",
    "    match = []\n",
    "    for n in range(2, 7):\n",
    "        s1_tokens = word_tokenize(s1)\n",
    "        s2_tokens = word_tokenize(s2)\n",
    "        s1_ngrams = set([' '.join(s1_tokens[i:i+n]) for i in range(len(s1_tokens)-n+1)])\n",
    "        s2_ngrams = set([' '.join(s2_tokens[i:i+n]) for i in range(len(s2_tokens)-n+1)])\n",
    "        match.append(len(s1_ngrams.intersection(s2_ngrams)))\n",
    "    return match\n",
    "\n",
    "def tfidf_score(s1, s2):\n",
    "    \"\"\"Compute the weighted TF-IDF score between two strings\"\"\"\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf = vectorizer.fit_transform([s1, s2])\n",
    "    return (tfidf * tfidf.T).A[0,1]\n",
    "\n",
    "def sentiment_difference(s1, s2):\n",
    "    \"\"\"Compute the sentiment difference between two strings\"\"\"\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    s1_sentiment = sid.polarity_scores(s1)['compound']\n",
    "    s2_sentiment = sid.polarity_scores(s2)['compound']\n",
    "    return abs(s1_sentiment - s2_sentiment)\n",
    "\n",
    "# Load the data as pandas dataframe\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Compute the external features and store in numpy array\n",
    "external_features =[]\n",
    "for i, row in df.iterrows():\n",
    "    s1 = row['headline']\n",
    "    s2 = row['body']\n",
    "    external_feature=char_ngram_match(s1,s2)+word_ngram_match(s1,s2)+[tfidf_score(s1,s2)]+[sentiment_difference(s1,s2)]\n",
    "    if i %1000==0:\n",
    "        print(\"num:{}, external_feature:{}\".format(i,external_feature))\n",
    "    external_features.append(external_feature)\n",
    "    # You can add more features here as needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[52, 32, 10, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.20589999999999997], [43, 28, 10, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.8053], [31, 16, 5, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.13480000000000003], [31, 18, 6, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.20589999999999997], [69, 64, 29, 14, 6, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0073264573132805115, 0.8053], [58, 39, 15, 6, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.42350000000000004], [33, 19, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 1.1235], [39, 29, 18, 6, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.022997222930638134, 0.8053], [56, 32, 14, 6, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.13480000000000003], [32, 21, 7, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.20589999999999997], [57, 25, 9, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.8053], [50, 32, 16, 11, 7, 5, 3, 2, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0.0, 0.8053], [27, 13, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.2479], [66, 50, 20, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.20589999999999997], [46, 21, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 1.1235], [51, 21, 6, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.8053], [35, 23, 11, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.2479], [59, 35, 19, 8, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.8053], [42, 28, 10, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0, 0.13480000000000003], [39, 17, 6, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.01850266147079386, 0.8053]]\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "\n",
    "print(external_features[:20])\n",
    "print(len(external_features[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=external_features)\n",
    "df.to_excel('external_features.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        [0, 0, 1, 0]\n",
      "1        [0, 0, 1, 0]\n",
      "2        [0, 0, 1, 0]\n",
      "3        [0, 0, 1, 0]\n",
      "4        [0, 0, 1, 0]\n",
      "             ...     \n",
      "49967    [0, 0, 0, 1]\n",
      "49968    [0, 0, 0, 1]\n",
      "49969    [0, 0, 0, 1]\n",
      "49970    [0, 0, 0, 1]\n",
      "49971    [0, 0, 0, 1]\n",
      "Name: Stance, Length: 49972, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_features_df=pd.read_excel('external_features.xlsx')\n",
    "external_features=external_features_df.values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[52.0, 32.0, 10.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2059], [43.0, 28.0, 10.0, 3.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8053], [31.0, 16.0, 5.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1348], [31.0, 18.0, 6.0, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2059], [69.0, 64.0, 29.0, 14.0, 6.0, 3.0, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007326457313280512, 0.8053], [58.0, 39.0, 15.0, 6.0, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4235], [33.0, 19.0, 6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.1235], [39.0, 29.0, 18.0, 6.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02299722293063813, 0.8053], [56.0, 32.0, 14.0, 6.0, 3.0, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1348], [32.0, 21.0, 7.0, 3.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2059]]\n",
      "x_train [[42.0, 20.0, 6.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8229], [40.0, 34.0, 24.0, 16.0, 11.0, 7.0, 3.0, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.2079253133002315, 0.9418], [76.0, 55.0, 33.0, 22.0, 17.0, 9.0, 5.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.01149075939888086, 1.3827], [47.0, 24.0, 5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0378], [56.0, 41.0, 21.0, 11.0, 8.0, 5.0, 3.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004831696660137391, 0.9835], [43.0, 18.0, 6.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01248989048115621, 1.1335], [58.0, 43.0, 28.0, 20.0, 15.0, 10.0, 8.0, 6.0, 4.0, 3.0, 2.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.02707186172889649, 0.9954], [40.0, 37.0, 26.0, 17.0, 10.0, 6.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01531811830426263, 1.3059], [49.0, 21.0, 5.0, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4511], [65.0, 42.0, 20.0, 9.0, 4.0, 3.0, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006797240423470669, 0.7743]]\n",
      "[[0, 0, 1, 0], [0, 0, 1, 0], [0, 0, 1, 0], [0, 0, 1, 0], [0, 0, 1, 0], [0, 0, 1, 0], [0, 0, 1, 0], [0, 0, 1, 0], [0, 0, 1, 0], [0, 0, 1, 0]]\n"
     ]
    }
   ],
   "source": [
    "print(external_features[:10])\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#拆分数据集\n",
    "x_train,x_test,y_train,y_test=train_test_split(external_features,y,random_state=666)\n",
    "#特征标准化处理\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "std=StandardScaler()\n",
    "#对训练样本集机型特征标准化处理\n",
    "x_train_standard=std.fit_transform(x_train)\n",
    "#对测试样本集进行特征标准化处理，要注意这里不能fit了\n",
    "#测试集的z—score标准化用的均值和标准差是训练集的，所以不用在fit直接转换就行了\n",
    "x_test_standard=std.transform(x_test)\n",
    "print('x_train',x_train[:10])\n",
    "print(y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hw\\AppData\\Local\\Temp\\ipykernel_45940\\1176397609.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_train = torch.tensor(y_train, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.8885\n",
      "Epoch [2/10], Loss: 0.8870\n",
      "Epoch [3/10], Loss: 0.8868\n",
      "Epoch [4/10], Loss: 0.8867\n",
      "Epoch [5/10], Loss: 0.8867\n",
      "Epoch [6/10], Loss: 0.8866\n",
      "Epoch [7/10], Loss: 0.8866\n",
      "Epoch [8/10], Loss: 0.8866\n",
      "Epoch [9/10], Loss: 0.8866\n",
      "Epoch [10/10], Loss: 0.8866\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 定义 MLP 模型\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.softmax(out)\n",
    "        return out\n",
    "\n",
    "# 定义训练数据和标签\n",
    "X_train = torch.tensor(x_train_standard, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "# 定义模型和优化器\n",
    "model = MLP(input_size=22, hidden_size=64, output_size=4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, X_train.shape[0], batch_size):\n",
    "        # 获取 mini-batch\n",
    "        inputs = X_train[i:i+batch_size]\n",
    "        labels = y_train[i:i+batch_size]\n",
    "\n",
    "        # 向前传播和计算损失\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # 向后传播和更新参数\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # 每个 epoch 结束后输出一下损失值\n",
    "    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "\n",
    "# 保存模型\n",
    "torch.save(model.state_dict(), 'mlp_model.pth')\n",
    "\n",
    "# # 加载模型并预测\n",
    "# model = MLP(input_size=22, hidden_size=64, output_size=4)\n",
    "# model.load_state_dict(torch.load('mlp_model.pth'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00000000e+00 3.55326721e-16 8.56593984e-13 4.18955461e-11]\n",
      " [9.89915788e-01 4.42059718e-06 1.50961450e-05 1.00646978e-02]\n",
      " [1.00000000e+00 2.33253439e-13 1.98370678e-11 1.66451863e-09]\n",
      " ...\n",
      " [2.16790184e-07 4.71871031e-09 9.99999762e-01 2.93728863e-09]\n",
      " [1.13218505e-07 5.78832626e-09 9.99999881e-01 1.99991912e-09]\n",
      " [1.27802929e-02 2.66522075e-05 9.87134516e-01 5.85276794e-05]]\n",
      "Accuracy: 0.8848955415032418\n",
      "Confusion Matrix:\n",
      "[[2084    0  121    5]\n",
      " [ 184    0   13    2]\n",
      " [ 162    0 8958    0]\n",
      " [ 892    0   59   13]]\n"
     ]
    }
   ],
   "source": [
    "# 设置模型为评估模式\n",
    "model.eval()\n",
    "\n",
    "# 将测试数据转换为张量\n",
    "X_test = torch.tensor(x_test_standard, dtype=torch.float32)\n",
    "\n",
    "# 使用模型进行预测\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test)\n",
    "\n",
    "# 将预测结果转换为 numpy 数组\n",
    "y_pred = y_pred.numpy()\n",
    "\n",
    "# 打印预测结果\n",
    "print(y_pred)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# 将 softmax 输出转换为类别标签\n",
    "y_pred_class = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# 将 y_test 转换为类别标签（如果 y_test 是 one-hot 编码）\n",
    "y_test_class = np.argmax(y_test, axis=1)\n",
    "\n",
    "# 计算准确率和混淆矩阵\n",
    "accuracy = accuracy_score(y_test_class, y_pred_class)\n",
    "confusion_mat = confusion_matrix(y_test_class, y_pred_class)\n",
    "\n",
    "print('Accuracy:', accuracy)\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, concatenate\n",
    "\n",
    "# Define the input shape for each feature vector\n",
    "input_shape = (10,)  # Assuming each feature vector has 10 elements\n",
    "\n",
    "# Define the number of neurons in each MLP layer\n",
    "mlp_units = 32\n",
    "\n",
    "# Define the number of classes for classification\n",
    "num_classes = 4\n",
    "\n",
    "# Input layers for each feature vector\n",
    "input_1 = Input(shape=input_shape)\n",
    "input_2 = Input(shape=input_shape)\n",
    "input_3 = Input(shape=input_shape)\n",
    "\n",
    "# MLP layers for each feature vector\n",
    "mlp_1 = Dense(mlp_units, activation='relu')(input_1)\n",
    "mlp_2 = Dense(mlp_units, activation='relu')(input_2)\n",
    "mlp_3 = Dense(mlp_units, activation='relu')(input_3)\n",
    "\n",
    "# Concatenate the outputs of the MLP layers\n",
    "concatenated = concatenate([mlp_1, mlp_2, mlp_3])\n",
    "\n",
    "# MLP layer for the concatenated output\n",
    "output = Dense(num_classes, activation='softmax')(concatenated)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=[input_1, input_2, input_3], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Generate random data for demonstration\n",
    "# Replace this with your actual data\n",
    "x1 = np.random.rand(100, 10)\n",
    "x2 = np.random.rand(100, 10)\n",
    "x3 = np.random.rand(100, 10)\n",
    "y = np.random.randint(num_classes, size=(100,))\n",
    "\n",
    "# Convert the target labels to one-hot encoding\n",
    "y_one_hot = np.eye(num_classes)[y]\n",
    "\n",
    "# Train the model\n",
    "model.fit([x1, x2, x3], y_one_hot, epochs=10, batch_size=32)\n",
    "\n",
    "# Make predictions on new data\n",
    "# Replace this with your actual test data\n",
    "x1_test = np.random.rand(10, 10)\n",
    "x2_test = np.random.rand(10, 10)\n",
    "x3_test = np.random.rand(10, 10)\n",
    "\n",
    "predictions = model.predict([x1_test, x2_test, x3_test])\n",
    "print(predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
